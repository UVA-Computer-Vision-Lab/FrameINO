
# Frame In-N-Out: Unbounded Controllable Image-to-Video Generation (NeurIPS 2025)

[![Paper](https://img.shields.io/badge/arXiv-Paper-b31b1b?logo=arxiv&logoColor=white)](https://arxiv.org/abs/2505.21491)
[![Website](https://img.shields.io/badge/Project-Website-pink?logo=googlechrome&logoColor=white)](https://uva-computer-vision-lab.github.io/Frame-In-N-Out/)


We propose Frame In-N-Out, a controllable image-to-video generation framework where objects can enter or exit the scene along user-defined motion trajectories. Our method introduces a new dataset curation pattern recognition, evaluation protocol, and a motion-controllable, identity-preserving video Diffusion Transformer, to achieve Frame In and Frame Out in the cinematic domain.





## <a name="Update"></a>Update ðŸ”¥ðŸ”¥ðŸ”¥
- [x] Release the paper
- [ ] Release the model weights (CogVideoX)
- [ ] Gradio demo (with online)
- [ ] Release the Training Code
- [ ] Release the Processed Training Dataset




## <a name="Intro"></a> Brief Intro Video ðŸ‘€
---

https://github.com/user-attachments/assets/0fabd2a4-9d3b-4148-bc04-6fc03c53caca

---
